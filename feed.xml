<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://shuvom-s.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://shuvom-s.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-07T02:15:21+00:00</updated><id>https://shuvom-s.github.io/feed.xml</id><title type="html">Shuvom Sadhuka</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Tracing My Internet Footprint and Vibe Coding</title><link href="https://shuvom-s.github.io/blog/2025/vibe-coding-and-traceroute/" rel="alternate" type="text/html" title="Tracing My Internet Footprint and Vibe Coding"/><published>2025-04-07T21:01:00+00:00</published><updated>2025-04-07T21:01:00+00:00</updated><id>https://shuvom-s.github.io/blog/2025/vibe-coding-and-traceroute</id><content type="html" xml:base="https://shuvom-s.github.io/blog/2025/vibe-coding-and-traceroute/"><![CDATA[<p>For an (alleged) computer scientist, I don’t really enjoy coding. Coding occupies the same</p>]]></content><author><name></name></author><category term="blog-posts"/><category term="internet"/><category term="networking"/><category term="vibe-coding"/><summary type="html"><![CDATA[Where does all my internet traffic go?]]></summary></entry><entry><title type="html">Measuring Entropy</title><link href="https://shuvom-s.github.io/blog/2025/measuring-entropy/" rel="alternate" type="text/html" title="Measuring Entropy"/><published>2025-02-11T21:01:00+00:00</published><updated>2025-02-11T21:01:00+00:00</updated><id>https://shuvom-s.github.io/blog/2025/measuring-entropy</id><content type="html" xml:base="https://shuvom-s.github.io/blog/2025/measuring-entropy/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/entropy-480.webp 480w,/assets/img/entropy-800.webp 800w,/assets/img/entropy-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/entropy.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Is the entropy of generations from a large language model an overestimate or underestimate of the true entropy of human language? How would one even measure the entropy of human language? The question itself is poorly defined, but in this post I’ll try to tackle what it might mean to measure the entropy of a language.</p> <h2 id="a-basic-primer-on-entropy-and-previous-works">A Basic Primer on Entropy and Previous Works</h2> <p>To start, let’s recall the definition of entropy (for discrete variables):</p> \[H(X) = -\sum_{x \in A} p(x) \log_2 p(x)\] <p>Intuitively, entropy measures the amount of uncertainty present in a random variable $X$. The highest entropy distribution over $X$ would be the uniform distribution, and the lowest entropy would be a point mass (with 0 entropy, since $\log(1) = 0$). Distributions that are very sharply centered, for instance, will have a $\log_2$ term closer to 0 near their center of mass.</p> <p>So if we want to measure the entropy of language, we’ll need to define two things: $A$, an alphabet we’ll measure over, and $p(x)$, a distribution over that alphabet. $A$ is fairly easy to define: the two natural approaches would be to let $A$ be characters (a, b, c, etc.) or words/tokens (the, ball, at, etc.). Let’s take $A$ to be words/tokens (like LLMs) for this post. The hard part is in estimating $p(x)$. We’ll call our estimate $q(x)$.</p> <p>Most naive approaches for measuring entropy of a language will likely overestimate the true entropy. For instance, one approach to estimate $p(x)$ would be to just use the empirical frequency of all words, but this would certainly overestimate entropy, because sentences are not constructed as sequences of random samples from the universe of words (e.g., “I is ball you” is not a valid sentence).</p> <p>A natural extension might be to consider 2-grams instead of 1-grams. Let’s measure the entropy of $p(x_1,x_0)$, for instance. This, too, will be an overestimate, because there are times when conditioning on the previous word is insufficient. For instance, every 2-gram in the sentence “I am red meat for you are here or there” is valid, but the sentence is nonsensical.</p> <p>In fact, the cross-entropy between any approximated model $q(x)$ like above and the true $p(x)$ will be an upper bound on the true entropy of $p(x)$. This is true mathematically because:</p> \[H(p,q) = H(p) + D_KL(p||q)\] <p>and KL divergence is non-negative. Cross entropy between $q(x)$ and $p(x)$ can be estimated empirically with text samples from a corpus (i.e., you get empirical samples from $p(x)$ and compare to your estimates $q(x)$).</p> <p>I’ve been slightly abusing notation here; technically the entropy we want to measure here is over a sequence, i.e. we want to measure</p> \[H(X) = -E_p \log P(x_0|x_{-1}, x_{-2}, …)\] <p>In other words, how much entropy is there in the distribution of the next word if I know all preceding words? These ideas and arguments have been thoroughly explored before, starting all the way from Claude Shannon himself in 1951 (shortly after he defined entropy in an earlier paper). I’d encourage you to read <a href="https://www.princeton.edu/~wbialek/rome/refs/shannon_51.pdf">his paper</a> — it’s not very long and full of interesting insights.</p> <p>The key insight is that if we let $F_n = H_n - H_{n-1}$, i.e. the conditional entropy of the n-gram model given the (n-1)-gram model, then we get that $H(X) = \lim_{n \rightarrow \infty} F_n$. In practice, of course, there are no $\inf$-grams, so Shannon calculates $F_n$ for small $n$ and tries to extrapolate the curve to predict the limit. Nowadays, we can probably easily compute 7-, 8-, and 9-grams on a standard laptop using a large corpus of text, which could give us a better sense of this limit. This also relates to a more general theorem known as the Shannon-McMillan-Breiman theorem, which states that for discrete-time finite-valued (both true here) stochastic processes:</p> \[H(X) \overset{p}{=} \lim_{n \rightarrow \infty} -\frac{1}{n} p(x_1, …, x_n)\] <p>There’s a catch to the theorem above that is worth noting. The theorem only holds for stationary ergodic processes (i.e., there exists a stationary distribution), but it’s clear that English (or more generally any language) is constantly evolving, as new words are added, old words become obsolete, and writing styles evolve. The entropy of English today will change tomorrow, because new words are invented all the time, for instance.</p> <h2 id="a-thorny-qualm">A Thorny Qualm</h2> <p>The above approximation is neat but something still feels wrong to me. In some sense, the entropy of English is lower bounded by the number of plausible ideas there are in the world. That is, there are two layers in language, each of which has its own entropy:</p> <ol> <li> <p>The generation of ideas in our heads, which we can denote $p(\text{idea})$. Different people think about different ideas, so there’s significant heterogeneity across our population in $p(\text{idea})$.</p> </li> <li> <p>Expressing in words the abstract idea we’ve generated, which we can denote $p(x|\text{idea})$.</p> </li> </ol> <p>This might seem like a triviality, but I’d argue it’s important to break this down. The entropy of ideas is (probably) invariant across languages. Thus, the real quantity of interest (to me) is (2) — how much entropy is there in the valid ways to express an idea. Combining (1) and (2) into one step and just measuring entropy of expressed sentences (as Shannon did) risks two issues:</p> <ol> <li> <p>Certain languages have become the <em>lingua franca</em> of specific professions. For instance, one may accidentally observe more entropy in English than in Swahili, Italian, or Hindi, simply because English is the international language of scientific research, which is in the business of generating new ideas and hypotheses (hence likely higher entropy).</p> </li> <li> <p>The translation between ideas and expressions in (2) is imperfect. Anyone who is a bilingual speaker is likely to be familiar with this. For instance, I often find it more difficult to express complex thoughts in Bengali (the language I speak with some family) than English (my native language).</p> </li> </ol> <p>Measuring $p(x|\text{idea})$ is hard, though, because there’s no clean notion of what an “idea” is. We can also argue about the correct resolution of an idea. For instance, are the sentences <em>I am eating the sandwich</em>, <em>I am devouring the sandwich</em>, and <em>I am nibbling on the sandwich</em> all expressing the same idea? It depends on what resolution you’re looking at. All these sentences express some form of consuming a sandwich, but they differ in how vigorously that consumption is happening.</p> <p>The space of ideas is (probably) continuous, but to make progress we need some way of mapping out that space. I had this conversation with a couple friends last year — how can we create an embedding space for ideas? I’m not sure, but I think people are approaching aspects of this question in the LLM world.</p> <p>Let’s imagine we have a perfect idea embedding model. To measure entropy of $p(x|\text{idea})$, we could cluster (i.e., discretize) the space of ideas at some resolution and then measure $p(x|\text{idea})$, perhaps using some of the techniques above. There’s still a question of how to measure $p(x|\text{idea})$. One way might be to use a generative language model, by using some kind of decoder on the idea embeddings. This is not that far off; we can already measure the entropy of $p(x|\text{prompt})$ in most language models, many of which are decoder-only.</p> <p>But $p(x|\text{idea})$ as measured by the outputs of some decoder is probably going to be an underestimate of the true entropy. There are a couple reasons for this:</p> <ol> <li> <p>Most language models are designed to be safe (i.e., outputs are not toxic, false, etc.), which necessarily constrains the output space of the approximation $q(x|\text{idea})$ to be more concentrated than the true $p(x|\text{idea})$. However, people often do express themselves in unsavory ways like swearing or being impolite that are still valid expressions of their ideas. To be clear, I am not advocating for unsafe language models, I’m just noting that LLM safety likely lowers the output entropy.</p> </li> <li> <p>Language models probably interpolate and average the training data in some way, so wildly outlier $p(x|\text{idea})$ are unlikely to appear. I’m not sure increasing the sampling temperature would help significantly, though I’d be happy to see what results people have in this direction.</p> </li> </ol> <h2 id="final-remarks-and-connections-to-other-domains">Final Remarks and Connections to Other Domains</h2> <p>I’ve thought of variants of the above questions in human genetics as well. If we wanted to estimate the entropy of the human genome, a naive lower bound would be something like $\log_2(\text{total humans who have ever lived})$ and a naive upper bound would be $\log_2(4^{3 \text{billion}})$ since there are 3 billion base pairs in the human genome.</p> <p>Both of these numbers are wildly off, though. The lower bound is clearly off because it changes every time someone is born (and it’ll be a long, long time before we observe two people with identical genomes). The upper bound is off because many genes (e.g., heavily conserved genes) are under heavy evolutionary pressure, and a small string of mutations would kill the person. The observed variation in the human genome is smaller than 3 billion. For instance, the 1000 Genomes Project has catalogued <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC9408407/">81 million SNPs</a> observed across 2,504 individuals. Even though 2,504 is a relatively small number, it tells us that most of the genome does not have significant variation across the population (e.g. minor allele frequency &gt; 1%).</p> <p>Genomes come with the additional twist that unlike English, the N-gram model would need to have an enormously long context to begin to understand the scaling of entropy as a function of $N$. Still, though, I think it could be useful to look at the entropy of 1- to 10-grams in DNA sequences (or similar) and see what the trend looks like. Perhaps a future post.</p> <p>Lastly, I want to note that I think the ideas point might have more salience. For instance, language models are often measured by their perplexities, with low perplexity indicating better performance. However, it’s unclear to me if higher perplexity is bad if the high perplexity comes from different generations that are semantically equivalent, even if the words are different (e.g., <em>The President of the United States is Donald Trump</em> and <em>Trump is the American president</em>).</p>]]></content><author><name></name></author><category term="blog-posts"/><category term="statistics"/><category term="language-models"/><category term="entropy"/><category term="genomics"/><summary type="html"><![CDATA[How would you measure the entropy of natural language?]]></summary></entry><entry><title type="html">Fellowship Applications</title><link href="https://shuvom-s.github.io/blog/2024/advice-on-applying-to-fellowships/" rel="alternate" type="text/html" title="Fellowship Applications"/><published>2024-10-21T21:01:00+00:00</published><updated>2024-10-21T21:01:00+00:00</updated><id>https://shuvom-s.github.io/blog/2024/advice-on-applying-to-fellowships</id><content type="html" xml:base="https://shuvom-s.github.io/blog/2024/advice-on-applying-to-fellowships/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/nsf-480.webp 480w,/assets/img/nsf-800.webp 800w,/assets/img/nsf-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/nsf.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Applying to fellowships is quite a daunting task. When I was applying, it felt impossible to write about the work I would do as a PhD student when I — then an undergraduate — didn’t even know where I’d physically be in a year. The dearth of resources online and hodgepodge of sometimes conflicting advice I received from friends and mentors only worsened the problem.</p> <p>I took the strategy of applying to nearly every fellowship under the sun and seeing what stuck. I don’t necessarily recommend that, but I also encourage those reading to apply to as many as they feel comfortable. Beyond the external funding, applying to fellowships really helped me clarify a coherent research question before starting my PhD. Although I ended up going in a different direction than what I wrote, it was a great exercise in writing coherently and cogently about a research problem.</p> <p>I hope this post clarifies (a) the high-level strategies I tried to follow during fellowship applications, and (b) some more specific personal advice on individual fellowships.</p> <h2 id="what-is-a-fellowship-and-why-does-it-matter">What is a fellowship and why does it matter?</h2> <p>When I started looking at fellowships, it was unclear to me what a fellowship even was or whether they made any tangible difference in your graduate studies. I looked online and found intimidating resources like this <a href="https://github.com/chinasaokolo/csGraduateFellowships">GitHub page</a> which had more fellowships listed than total faculty in some CS departments. Some of them I was eligible for and some not, but organizing the pile of opportunities was (quite literally) a tall task.</p> <p>In short, fellowships provide <em>external</em> funding to cover your tuition and some or all of your graduate stipend.</p> <p>For example, if you receive an NSF fellowship, NSF would pay your tuition (typically covered by your advisor) to MIT and a stipend which is typically below the standard rate for your department. Your department or advisor would pay the difference and (perhaps) a small bonus to reward you for winning the fellowship. So, if my departmental tuition were \$100,000 a year and my stipend \$50,000 then NSF would pay the full \$100,000 and \$35,000 to MIT. MIT would forward me this \$35,000 as well as, for instance, an extra \$20,000 to bring my total stipend to \$55,000, slightly above the standard rate. (It’s worth noting that these numbers vary by department and program, and not every department pays above the standard rate, even with a fellowship.)</p> <p>External funding provides additional flexibility. For instance, I was fortunate to get a couple fellowships. Between them, I was able to navigate the graduate school selection process (post admission) with some peace of mind knowing that I wouldn’t have funding stress anywhere. This steered the conversations with potential advisors away from “do you have funding” and “how many students can you support next year” to “what topics are you thinking about nowadays” and “are there new research directions in your group?” To be clear, you should ask all of these questions regardless, but the fellowship can help you prioritize the more scientifically grounded concerns.</p> <p>I’ve also heard of friends on fellowships who have found it easier to collaborate with researchers at other institutions (as your funding is not directly tied to your home institution). Lastly, the fellowships do provide a CV boost and potentially some networking opportunities.</p> <h2 id="what-are-some-strategies-i-should-keep-in-mind">What are some strategies I should keep in mind?</h2> <p>While I originally tried to write a one-size-fits-all essay, I quickly found that I had to scrap and rewrite portions of each essay to tailor it to the fellowship. The best resource I had were mentors — other PhD students and advisors — who themselves had gone through similar processes. Give them sufficient heads up and ask them to tear through your work. It was disheartening for me to know that some of the scientific ideas (e.g. “representation learning for biological sequences”) I had were too vague or imprecise, but the back and forth of editing helped me clarify the writing and my own ideas. I even caught up to the state-of-the-art in the field of computational genomics and AI for biology.</p> <p>After my first round of messy edits, I started jotting down a few items for each fellowship:</p> <ol> <li>Who will be reviewing my application? What level of technical depth is appropriate?</li> <li>What personal qualities or traits are emphasized in this program? What words or phrases are highlighted on their website that flag specific qualities?</li> <li>What types of people typically win this award? Is their work heavily interdisciplinary or field specific?</li> </ol> <p>It’s great to have a rough template of ideas and essays common across fellowships, but it’s equally important to tailor your application. I found that answering the questions above provided an appropriate starting point for my applications. I knew that the DOE fellowship was focused on the high-performance computing aspects of my work, while for the NSF application I could emphasize the intrinsic value of the scientific questions raised by my proposal. I also let my mentors know how the fellowships differed in their focuses based on these questions.</p> <p>The above three questions are the most universal advice I can give. Each of the fellowship-specific tips I provide below are based on the questions above.</p> <h2 id="nsf-grfp">NSF GRFP</h2> <p><strong>Eligibility</strong>: US citizen or permanent resident, can apply once as an undergraduate and once as a graduate student</p> <p><strong>Timeframe</strong>: 3 years</p> <p><strong>Advice</strong>: Fortunately NSF is one of the few fellowships for which there are high-quality publicly available resources. The best resource is Alex Lang’s website, which contains over a decade’s worth of winning applications. NSF is probably the closest one to a grant proposal (or so I imagine), and I recommend providing the nitty gritty scientific details of your project. It’s also important to develop a clear, concise, yet well-scoped proposal that relates to your previous research. To this end, you should not have proposals as broad as “Develop ML Models Robust to Distributional Shifts” or as narrow as “Quantify Failure Modes of Diffusion Models Under ABC Rotational Transformations in Dataset XYZ.” Somewhere in the middle is the sweet spot, and the website can help you adjust.</p> <h2 id="ndseg">NDSEG</h2> <p><strong>Eligibility</strong>: US citizen or permanent resident, have at least three years remaining in your graduate students</p> <p><strong>Timeframe</strong>: 3 years</p> <p><strong>Advice</strong>: This one is a bit quirky in that they ask you to scan through their specific research goals (“Broad Agency Announcements”) and find one that suits your project. Make sure you choose wisely, because they certainly will assess your application at least partially based on the relevance of your work to this goal. Other than that, I’d generally emphasize how your research may enhance defense capabilities, even if you have to stretch your mind a bit. <em>Do not just copy-paste your NSF essay and add the defense relevance at the end as I did</em>; instead try to reframe your essay to highlight the relevance to defense throughout.</p> <h2 id="doe-csgf">DOE CSGF</h2> <p><strong>Eligibility</strong>: Undergraduate seniors and first-year PhD students who are US citizens or permanent residents</p> <p><strong>Timeframe</strong>: 4 years</p> <p><strong>Advice</strong>: This one is the most mysterious to me, but I can tell they care deeply about the coursework, so do not take that part of the application lightly! Again, try to reframe your essay to highlight its energy relevance — perhaps your work will enhance energy efficiency or provide new energy storage methods.</p> <h2 id="hertz-foundation">Hertz Foundation</h2> <p><strong>Eligibility</strong>: Undergraduate seniors and first-year PhD students who are US citizens or permanent residents</p> <p><strong>Timeframe</strong>: 5 years</p> <p><strong>Advice</strong>: The Hertz Fellowship provides the most funding (5 years) and some networking opportunities. The Hertz Foundation spells out the values it cares about on its website: creativity, leadership, and innovation. Of the fellowships I applied for, I spent the most time contemplating how to frame my Hertz essays to highlight these traits. As someone who didn’t start finding interest in anything remotely biological until sophomore year of college, I made sure to emphasize how my somewhat unconventional journey into computational biology strengthened my work by providing a broad toolkit.</p> <p>As for the interview, I found practicing with mentors and friends to be most helpful. I explained my work to those around me at different levels: in the technical weeds, high-level picture, and elevator pitch. I also ran a couple practice interviews with my advisor, who asked me to explain a number of technical concepts (e.g. explain a VAE, what is Hardy-Weinberg equilibrium, etc.). It’s acceptable and even encouraged to get flustered when you practice; in my opinion, the best preparation for the technical interview is to practice explaining thorny scientific concepts clearly.</p> <h2 id="soros">Soros</h2> <p><strong>Eligibility</strong>: Child of an immigrant or an immigrant</p> <p><strong>Timeframe</strong>: 2 years</p> <p><strong>Advice</strong>: I actually didn’t end up applying for Soros, mostly because I was swamped with other applications. That’s totally fine! While I did not apply, Soros awards fellowships to all types of graduate students: law, medicine, business, science, etc. As such, you should focus on broader themes of how your immigrant experiences shaped your work. I’d recommend focusing less on your specific research and instead trying to highlight your “immigrant” and “American” values that they mention on their website (e.g. hard work, innovation, creativity, cultural fusions, etc.).</p> <h2 id="other-fellowships">Other fellowships</h2> <p>There are so many other fellowships, and I’d recommend applying to as many as you can. Some prominent ones include Knight-Hennessy (Stanford only), Quad (US, India, Japan, etc.), GEM, etc. There are also CS-specific ones sponsored by big tech companies, such as Google, IBM, Facebook, etc. The CS-specific ones often require departmental sponsorship and are only open to PhD students in the latter years of their programs. While I’m not so familiar with non-CS fellowships, I’m sure there are others you can find. Lastly, please remember that it’s totally fine to not be on a fellowship. MIT, for instance, covers tuition and provides a stipend to all students for their first year, regardless of advisor status, and this is common across many programs and universities. Afterwards, your advisor will cover funding (and feel free to clarify this with your advisor before joining). Good luck!</p>]]></content><author><name></name></author><category term="blog-posts"/><category term="fellowships"/><category term="advice"/><category term="science"/><summary type="html"><![CDATA[Advice on applying to fellowships]]></summary></entry><entry><title type="html">A running list of writing ideas</title><link href="https://shuvom-s.github.io/blog/2024/my-thoughts/" rel="alternate" type="text/html" title="A running list of writing ideas"/><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://shuvom-s.github.io/blog/2024/my-thoughts</id><content type="html" xml:base="https://shuvom-s.github.io/blog/2024/my-thoughts/"><![CDATA[<p>Here are some ideas I have that probably aren’t worthy of a full research project (or I don’t have the expertise to do them) but I think are interesting.</p> <h2 id="markets">Markets</h2> <ul> <li>Calibration of prediction markets <ul> <li>Prediction markets have become very popular in the past couple years. Are they actually calibrated? At what time horizons? How do they compare to general prediction tools?</li> </ul> </li> <li>How to evaluate investment portfolio strategies <ul> <li>Is there a way to establish statistical significance of A vs B strategy?</li> <li>What is the null distribution of returns? How can we construct a null distribution of strategies?</li> <li>Should we use the integral between A and B as a metric? Should we only use a single point? What time horizons should we use?</li> </ul> </li> </ul> <h2 id="personalrandom">Personal/Random</h2> <ul> <li>Analyzing songs through embedding APIs and/or SAEs <ul> <li>What themes are most important in different song genres?</li> </ul> </li> <li>Analyze my own bike trips <ul> <li>I have lots of Strava data</li> </ul> </li> </ul> <h2 id="social-choice-and-economcs">Social Choice and Economcs</h2> <ul> <li>How can we aggregate human preferences? <ul> <li>How can we align ML systems if we can’t aggregate human preferences properly?</li> </ul> </li> </ul> <h2 id="biology-and-health">Biology and Health</h2> <ul> <li>What is the entropy of the human genome?</li> <li>Do people actually make better decisions when given calibrated advice? Do doctors? Should we only use calibrated models in clinical decision making?</li> <li>What is the full distribution of SNP effect sizes? Are there ways to collect and interpret SNP effect sizes without using linear models?</li> <li>How can we assess if polygenic risk scores are actually useful? If we tell people their PRS and high-risk individuals don’t develop disease X (e.g., a cardiovascular disease), is it because the PRS was wrong or because they positively changed their behavior?</li> <li>For a long time, bulk RNA-seq was the gold standard for measuring gene expression. After that came single-cell RNA-seq. Now we have access to spatial transcriptomics. If we could design a new method for measuring gene expression (without any constraints), what measurements should we prioritize?</li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="random"/><summary type="html"><![CDATA[Things I want to write (and read) about]]></summary></entry><entry><title type="html">Overcoming the False Trade-off in Genomics</title><link href="https://shuvom-s.github.io/blog/2023/overcoming-the-false-tradeoff-in-genomics/" rel="alternate" type="text/html" title="Overcoming the False Trade-off in Genomics"/><published>2023-06-01T00:00:00+00:00</published><updated>2023-06-01T00:00:00+00:00</updated><id>https://shuvom-s.github.io/blog/2023/overcoming-the-false-tradeoff-in-genomics</id><content type="html" xml:base="https://shuvom-s.github.io/blog/2023/overcoming-the-false-tradeoff-in-genomics/"><![CDATA[<p>Note: This post was originally published for MIT’s <a href="https://computing.mit.edu/wp-content/uploads/2023/06/Overcoming.pdf">Envision the Future of Computing Prize</a>, where it won an honorable mention.</p> <p>On June 26, 2000, President Bill Clinton and Prime Minister Tony Blair jointly announced to the world that the first draft of the human genome had been completed. Speaking with unfettered optimism on the implications of Human Genome Project (HGP), President Clinton <a href="https://archive.nytimes.com/www.nytimes.com/library/national/science/062700sci-genome-text.html">declared</a>:</p> <blockquote> <p>“Without a doubt, this is the most important, most wondrous map ever produced by humankind. […] It will revolutionize the diagnosis, prevention, and treatment of most, if not all, human diseases. […] In fact, it is now conceivable that our children’s children will know the term cancer only as a constellation of stars.”</p> </blockquote> <p>Two decades later, the president’s lofty promises have proven to be somewhat prescient. There is little doubt that whole genome sequencing — now more than five orders of magnitude cheaper than in 2000 — has revolutionized biomedicine.</p> <p>Pharmacogenomics is already empowering precision medicine targeted to the genetic mutations a patient carries. While a blanket cure for cancer as President Clinton envisioned is yet to be realized, CRISPR gene editing technologies have also generated considerable scientific excitement for the next era of therapeutic medicine. Even long-held anthropological questions regarding human migration can be <a href="https://www.nytimes.com/2018/04/20/books/review/david-reich-who-we-are-how-we-got-here.html">answered</a> through computational analysis of ancient DNA. These innovations are not limited to academia either. Today, there are numerous publicly traded companies (e.g. <a href="https://www.23andme.com/">23andMe</a>) whose core products involve sequencing or analysis of genomic data.</p> <p>Each of these efforts has <em>only</em> been made possible through large international collaborations since the HGP, and further breakthroughs in genomics will inevitably continue to change the pace of drug discovery and biomedical innovation. Indeed, collaboration is at the heart of genomics and biomedical research: groundbreaking discoveries only occur when data is pooled across multiple ethnicities, conditions, and backgrounds.</p> <p>However, buried within Clinton’s speech were other concerns about the future of genomics:</p> <blockquote> <p>“As we unlock the secrets of the human genome, we must work simultaneously to ensure that new discoveries never pry open the doors of privacy. And we must guarantee that genetic information cannot be used to stigmatize or discriminate.”</p> </blockquote> <p>President Clinton’s words were especially important given the historically fraught relationship between population genetics and minority communities. Some of the original titans of the field and inventors of mathematical tools for genetic analysis manipulated their inventions to promote scientific racism. Absent proper safeguards, access to the highest resolution information about human ancestry, DNA, can be used for novel forms of discrimination and surveillance.</p> <p>As international collaborations explode in size, concerns regarding genomic privacy and ethics are growing; in the past two years, US Congress has debated the federal <a href="https://www.rubio.senate.gov/public/index.cfm/2021/5/rubio-introduces-bills-to-counter-growing-threat-of-china-s-collection-of-american-s-genomic-data">Genome Data Security Act</a> and California has signed the <a href="https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202120220SB41">Genetic Information Privacy Act</a> into law. While these laws aim to protect genomic data, it is unclear exactly what kind of attacks and analyses may be possible in the event of a future privacy breach. As our understanding of the human genome continues to evolve, richer biometric information can be extracted from genomic samples. For instance, prenatal genetic testing can <a href="https://www.acog.org/womens-health/faqs/prenatal-genetic-screening-tests">illuminate</a> disease risks prior to birth.</p> <p>However, we do know two key differences in genomic privacy and ethics concerns compared to other forms of data: (1) unlike credit card numbers or social media, the human genome is <em>immutable</em>, so a one-time leak of information is a lifetime leak, (2) genomes are strongly correlated between relatives, so a leak has privacy implications beyond the individual, for families and even communities.</p> <p>Traditionally, stronger privacy regulations are considered at odds with collaborative research and development. However, <strong>this is simply not an option in genomics</strong>, where strong restrictions will prevent life-saving therapeutics from being developed.</p> <p>In this essay, I argue for the need to develop new technologies that both enhance genomic privacy <em>and</em> foster large international collaborations. Specifically, I argue that the oft-repeated trade-off between privacy and utility is a false dichotomy that can be overcome in genomics with significant engineering and legal effort. We must develop two forms of data security and privacy to enable such collaborations:</p> <ol> <li><strong>Institutional data security</strong>, or security of large-scale biological data repositories, hospitals, and corporations against malicious actors hoping to steal or manipulate this data.</li> <li><strong>End-user data privacy</strong>, or guarantees to patients participating in studies that their individual data will not be identifiable or inappropriately shared.</li> </ol> <p>Importantly, strictly “legislating” around this privacy issue to institute tighter data access measures will only slow the pace of research and play into the trade-off. Instead, we must lean on new computing technologies to maintain scientific efficiency while promoting security, privacy, and compliance.</p> <h2 id="a-new-era-of-collaborative-research">A New Era of Collaborative Research</h2> <p>Collaborative scientific research is a prerequisite for genomics. Genetic signatures of diabetes, for example, can differ from population to population, so generalizable and statistically significant findings require pooling data from organizations worldwide. This is reflected in the endlessly growing list of consortia dedicated to the study of biomedical questions through genomics: <a href="https://www.ukbiobank.ac.uk/">UK Biobank</a>, <a href="https://allofus.nih.gov/">NIH All of Us</a>, <a href="https://www.finngen.fi/en">FinnGen</a>, and so on.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/genomecost-480.webp 480w,/assets/img/genomecost-800.webp 800w,/assets/img/genomecost-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/genomecost.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The cost of sequencing a human genome has dropped precipitously. Large databases now contain thousands of genomes. (Figure credit: NHGRI) </div> <p>The change is being acknowledged at all levels of biomedical research. President Biden’s cancer moonshot goal — started in his Vice President years — to reduce the rate of cancer deaths by half in twenty-five years includes a project to develop a cloud-based cancer genomic data analysis platform authored by Microsoft, Amazon Web Services, and the National Cancer Institute. Closer to MIT, the Broad Institute is partnering with Microsoft to develop <a href="https://www.microsoft.com/en-us/research/blog/biomedical-research-platform-terra-now-available-on-microsoft-azure/">Terra</a>, a secure and efficient cloud-based biomedical data analysis platform.</p> <p>Researchers, clinicians, and industry partners are using these databases and platforms to improve precision medicine. For example, the Cancer Genome Atlas has gathered more than 2.5 petabytes of data, which has been used to identify key cancer genes like BRCA1 and provide more fine-grained diagnoses beyond the standard stages of cancer.</p> <p>Kemal Malik, director of innovation for Bayer, <a href="https://www.nationalgeographic.com/science/article/partner-content-genomics-health-care#:~:text=Genomics%2C%20the%20study%20of%20genes,as%20height%20and%20hair%20color.">highlights</a> the crucial role genomics plays in precision medicine and disease treatment:</p> <blockquote> <p>“The Holy Grail in health care has long been […] precision medicine. But getting to the level of precision we wanted wasn’t possible until now. What’s changed is our ability to sequence the human genome […] To date, genomics has had the most impact on cancer because we can get tissue, sequence it, and identify the alterations […] In the future we’ll see every cancer patient sequenced, and we’ll develop specific drugs to target their particular genetic alteration.”</p> </blockquote> <p>Indeed, the <em>key</em> driver of precision medicine has been pharmacogenomics. Under this new clinical paradigm, drugs are prescribed only if the predicted patient response (based on genomic information) is positive. Pharmacogenomics is not a pipe dream: since 2011, the FDA has <a href="https://www.fda.gov/media/124784/download">labeled</a> 250+ prescriptions with dosage recommendations based on genetic differences. For instance, the breast cancer drug PIQRAY was shown to be more effective in patients with a mutation to the PIK3CA gene than those without one. As we develop a more nuanced understanding of pharmacogenomics, it’s possible — even likely — that we will move away from FDA-curated labels to FDA-approved drug recommendation software. Some <a href="https://genomind.com/">startups</a> are already <a href="https://genesight.com/">testing</a> these <a href="https://www.genxys.com/">waters</a>.</p> <p>In a related development, CRISPR technologies are moving the needle on gene editing, which could help cure once incurable diseases. After a series of publications in 2012 demonstrating that CRISPR/Cas9 could edit genes through precise cuts and natural repairs to DNA sequences, CRISPR has captured the imagination of biotechnologists worldwide. Vertex Pharmaceuticals has already <a href="https://www.nytimes.com/2022/12/09/opinion/crispr-gene-editing-cures.html">cured</a> 31 people of sickle cell disease, and other companies are testing preclinical drugs. Scientists have also been using CRISPR to develop tests for infectious diseases at lower costs than PCR tests and even extreme-weather resistant crops.</p> <p>Genomics has even answered questions in social sciences. Geneticists like Svante Pääbo (2022 Nobel Laureate) have leveraged computational techniques to unlock age-old questions about human migration. By mapping and comparing fragments of DNA found in ancient human bones, they have been able to <a href="https://www.nytimes.com/2018/04/20/books/review/david-reich-who-we-are-how-we-got-here.html">establish</a> new lineages in the human story, like the archaic humans from the Denisovan caves, whose genomes account for up to 5% of some modern lineages.</p> <h2 id="characterization-of-privacy-risks">Characterization of Privacy Risks</h2> <p>In parallel with the scientific advances, the call for genomic privacy has grown from a mellow hum to a thunderous roar. It is natural to believe that the path forward for genomics is to remove barriers to collaboration, a route which has already provided numerous discoveries. However, without proper safeguards, the rift of trust between the public and medical science will expand. One can only imagine if baseless conspiracies — such as <a href="https://www.unicef.org/montenegro/en/stories/covid-19-vaccine-does-not-change-human-dna">claims</a> that COVID vaccines directly edit human DNA — are provided genuine substance in the event of a large-scale genetic privacy breach. Maintaining trust is one of many concerns, and as we expand these collaborations public trust must be paramount.</p> <p>Early privacy studies indicated that relatively few — just 75 — genomic coordinates (“single nucleotide polymorphisms”) were needed to uniquely identify many individuals, but it was unclear how such findings would generalize given the sparsity of large-scale sequencing databases. Re-identifying individual genomes at scale seemed akin to searching for a genetic needle in a haystack of DNA.</p> <p>At some point in the 2010s, the tone shifted. A flurry of papers demonstrating that anonymized genomic data could be linked to individuals or other phenotypic information raised alarm. One landmark <a href="https://www.science.org/doi/full/10.1126/science.1229566">study</a> in 2018 leveraged paternal surname inheritance to link anonymous genomes to specific names. By exploiting the Y chromosome — present only in males and thus, like surnames, typically inherited from the father — and a small (private) database of labeled genomes, researchers were able to triangulate the identity of a substantial fraction of males. Extrapolating from their results, they estimated that 12% of all European descent males in the US were susceptible to such an attack. Responding to the work, Eric Green, former director of the NIH Human Genome Research Institute, <a href="https://www.abajournal.com/news/article/researchers_identify_dna_study_volunteers_through_online_sleuthing_consent_">noted</a>, “we are […] an awareness moment.”</p> <p>Since the publication of this and similar attacks, new avenues of privacy breaches have been demonstrated, both in real-world and sandboxed settings. The attacks run the gamut of techniques, but some noteworthy examples include:</p> <ol> <li> <p><a href="https://pubmed.ncbi.nlm.nih.gov/34746296/">Reconstruction</a> of private genomes by repeatedly sending (legal) queries to a server.</p> </li> <li> <p>A <a href="https://www.washingtonpost.com/politics/2022/07/21/hacks-genetic-firms-pose-risk-patients-experts-say/#:~:text=Since%20the%20beginning%20of%20last,of%20Health%20and%20Human%20Services.">data breach</a> from servers of a company due to unencrypted data stored on a problematic server.</p> </li> </ol> <p>Unlike other forms of data, genomic data is immutable, so these attacks have implications well beyond the victim and the specific time of the attack. It is nearly impossible to think of another type of data which, if leaked, can reveal sensitive health information of the victim’s future great-grandchildren. Brad Malin, professor of computer science at Vanderbilt, <a href="https://www.washingtonpost.com/politics/2022/07/21/hacks-genetic-firms-pose-risk-patients-experts-say/">notes</a> that the risks are “highly dependent on how the adversary wants to use the data.” Feasible possibilities <a href="https://www.wired.com/story/the-us-urgently-needs-new-genetic-privacy-laws/">include</a> employment and housing discrimination based on genetics, which is legal in certain municipalities at the moment.</p> <p>In response, various government agencies have begun instituting tighter controls for genomic data. Last year, the National Institute of Standards and Technology (NIST) <a href="https://www.nccoe.nist.gov/projects/cybersecurity-and-privacy-genomic-data">announced</a> a new project to “identify genomic data cybersecurity and privacy concerns and develop guidance to address these challenges.” NIST’s project touches all levels of genomic analysis, from ensuring that sequencing devices themselves are secure to writing better access protocols for genomic data repositories.</p> <p>FBI agent Ed You <a href="https://www.ft.com/content/245a7c60-6880-11e7-9a66-93fb352ba1fe">notes</a> that “cross-border deals are not the only risks to US genetic data. The healthcare industry is notoriously vulnerable to cyberattacks.” These concerns have directly influenced legislation. <a href="https://www.techtarget.com/healthtechsecurity/news/366595025/Growing-Number-of-States-Enact-New-Genetic-Data-Privacy-Laws">Four states</a> have passed genetic privacy protections, and Congress is currently considering the Genomic Data Security Act, a bill which would place restrictions on Chinese access to American genomic data.</p> <h2 id="overcoming-the-privacy-utility-trade-off">Overcoming the Privacy-Utility Trade-off</h2> <h3 id="a-false-trade-off">A False Trade-off?</h3> <p>Typically, the tension between privacy and collaboration is framed as zero-sum. Former UK Health Secretary Matt Hancock bluntly <a href="https://www.gov.uk/government/speeches/we-must-tackle-the-serious-ethical-challenges-of-dna-analysis">noted</a> that: “it’s outrageous that too often, anonymised data […] can’t be used for research. We will unlock that data because […] it saves lives.” Hancock’s concerns are reasonable and highlight the cost of strict measures: patient lives. Nonetheless, the false dichotomy is dangerous and could lead us into a vicious cycle of removing access barriers only to later realize privacy issues. We must therefore overcome Hancock’s implicit belief that privacy and collaborative research are “too often” at odds.</p> <p>The previous attacks demonstrate a need for both <em>institutional data security</em> and <em>end-user genomic privacy</em> paradigmsto overcome this double bind. While the distinction is somewhat artificial, attacks stemming from unauthorized access fall more in an <em>institutional</em> oversight realm. On the other hand, reconstruction of a private genome through public queries seems to be an <em>end-user</em> privacy issue.</p> <p>The distinction is perhaps best explained by the services offered in the financial sector. If a bank liquidates, it is typically blamed for its poor <em>institutional</em> management. To compensate for this, <em>end users</em> can choose to only hold savings in FDIC-insured banks, so that their money is federally insured in case of mismanagement.</p> <p>Despite strict privacy regulations, a thriving market — fintech — exists to provide customers with better solutions and overcome a double bind. A similar balance is needed in genomics.</p> <h3 id="a-response-from-computing">A Response from Computing</h3> <p>Several computing technologies actively being developed at MIT could empower the secure and efficient genomic data collaboration needed to overcome the false trade-off between privacy and collaboration.</p> <p>On the institutional data security front, there are proof-of-concept and real-world applications of cryptographic tools to enable secure collaboration. As an instructive example, imagine there are three biobanks which would like to jointly analyze their data without revealing their private patient data.</p> <p>Secure multiparty computation (SMC) defines a set of protocols which enable joint computation of a function when the data is distributed across multiple parties. For example, if the function to compute is $F(x,y,z) = xyz$, then SMC allows the parties which separately own $x$, $y$, and $z$ to compute $F$ without revealing anything about their inputs. The key method behind SMC is known as secret sharing, which uses some elegant properties of polynomial interpolation to guarantee security.</p> <p>This is a promising paradigm for collaboration, as functions we would like to compute over distributed datasets could now be computed in cryptographically secure ways. <a href="https://www.nature.com/articles/nbt.4108">Researchers at MIT</a> have already shown proof-of-concept results for collaborative genomic analyses — such as gene-disease studies — using SMC without sharing raw patient data.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fhe-480.webp 480w,/assets/img/fhe-800.webp 800w,/assets/img/fhe-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/fhe.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> FHE allows direct computation on encrypted data. </div> <p>In a separate but similar setting, imagine a hospital holds a partial genomic sequence of a patient and would like to impute the rest. Genotype imputation is an expensive task, so the hospital asks a remote server to help with the imputation. Fully homomorphic encryption (FHE) allows direct computation on encrypted data. In this paradigm, the hospital <a href="https://www.sciencedirect.com/science/article/pii/S240547122100288X">sends</a> an encrypted genome to a remote server, which provides (an encrypted version of) the correct answer without learning the input.</p> <p>While FHE/SMC provide cryptographic security guarantees, engineering them to be efficient is challenging in practice. The additional memory requirements for these technologies makes computation exceedingly burdensome in many cases, and each line of the computer program further elongates the runtime. Limited network bandwidths can explode the total time required if the network traffic needed for the protocol is high. For these reasons, SMC/FHE are yet not used at scale in practice. These protocols are especially hard to implement for intrinsically resource-intensive computations like deep learning. We therefore <strong>must</strong> continue developing and deploying scalable SMC/FHE and related technologies to provide a bridge — cryptographically secure protocols for widespread collaboration — across the false trade-off between privacy and collaboration.</p> <p>On the <em>end-user</em> data front, methods like differential privacy are <a href="https://academic.oup.com/bioinformatics/article/36/6/1696/5614817">providing</a> stronger guarantees that individual patient data will not be leaked through some of the statistical attacks described earlier. Differential privacy — a technique that adds noise to any aggregated data releases — provides a mathematical framework to pinpoint the chance that a patient’s data can be re-identified from legal queries to database. Calibrating the correct noise to add is notoriously difficult: too little noise weakens privacy while too much noise can muddle important downstream analyses.</p> <p>The solution in applications thus far has been a tenuous consensus from experts and stakeholders. Even with such careful deliberations, institutions like the US Census have received <a href="https://www.washingtonpost.com/local/social-issues/2020-census-differential-privacy-ipums/2021/06/01/6c94b46e-c30d-11eb-93f5-ee9558eecf4b_story.html">criticism</a> for their noise levels.</p> <p>It is unlikely that any noise level — or any cryptographic protocol — will satisfy everyone’s preferred privacy level. Privacy is inherently personal: you may mind your friend snooping through your text messages, but your neighbor may not. The only truly trustworthy and long-term solution is to therefore offer fine-grained consent laws to users. These laws should enable users to specify who to share their data with and what analyses are allowed with their data. At the loosest level, no protections will be available beyond what is federally required. At the strongest level, the user can retract their data from a database and scrap any traces of it anywhere. An in-between level may include an authorization for use in collaborative studies, but only if technologies such as FHE/SMC are used.</p> <p>The specific formulations of consent will require the collaboration of patients, clinicians, researchers, and legislators, but the need for fine-grained privacy laws is clear.</p> <h3 id="a-look-to-the-future">A Look to the Future</h3> <p>Perhaps President Clinton will prove to be right, only several decades later. It is conceivable that certain rare diseases and cancers will only be historical afflictions at some point in the 21st century. As ambitious as that may seem, the 20th century brought miracle cures: a polio vaccine, the first antibiotics, and IVF babies. Unlocking the human genome will provide many more.</p> <p>As we look to this future with starry-eyed optimism, it is critical that we not get locked into an endless debate between privacy and utility. Instead, we must imagine new worlds — both technologically and through legislation — in which the two co-exist. Bridging the gap between privacy and collaboration is the only way forward to save as many lives as possible, and some of the ideas introduced here could lead the way. It is only through locking our genetic secrets that we can fully unlock our genome’s revolutionary potential.</p>]]></content><author><name></name></author><category term="blog-posts"/><category term="genomics"/><category term="privacy"/><category term="law"/><summary type="html"><![CDATA[Does privacy have to be at odds with collaboration in biomedical research?]]></summary></entry><entry><title type="html">On Voting Paradoxes</title><link href="https://shuvom-s.github.io/blog/2020/on-voting-paradoxes/" rel="alternate" type="text/html" title="On Voting Paradoxes"/><published>2020-05-22T00:00:00+00:00</published><updated>2020-05-22T00:00:00+00:00</updated><id>https://shuvom-s.github.io/blog/2020/on-voting-paradoxes</id><content type="html" xml:base="https://shuvom-s.github.io/blog/2020/on-voting-paradoxes/"><![CDATA[<p>Note: This post was originally published on <a href="https://randomquadwalks.com/2020/05/22/on-voting-paradoxes/">my old blog</a>.</p> <p>My post last week sent me down a Wikipedia/Google rabbit hole on voting systems and paradoxes. I’d heard in passing about voting paradoxes but never took some time to explore them. In this post, I’ll discuss how we can use graphs to model elections and then explore the ideas behind voting paradoxes.</p> <h2 id="i-a-graph-reduction-for-elections">I. A Graph Reduction for Elections</h2> <p>To start, let’s define some notation (note: a lot of the ideas here are from <a href="https://people.csail.mit.edu/rivest/gt/2010-05-22-RivestShen-AnOptimalSingleWinnerPreferentialVotingSystemBasedOnGameTheory_conf.pdf">this paper</a>, so check it out if you’re interested!). Suppose we have $m$ candidates, who we’ll call $c_1, \ldots, c_m$, and $n$ voters, with each voter casting a ballot ranking some or all of the candidates. Each ballot is an ordered set $B_i = {c_{(1)}, \ldots , c_{(m)}}$ corresponding to the voter’s preferences. We can then construct a collection $C$ of ballots ${B_1, \ldots, B_n}$ of everyone’s votes, which we’ll call the election profile. Our goal is to determine a “fair” method to select a winning candidate. If we choose to represent the election in matrix form, then let $M_{i,j} = \sum_{B_k \in C} I(B_k \text{ ranks i over j}) - I(B_k \text{ ranks j over i})$ (intuitively, it’s just the margin of voters who prefer $i$ to $j$). Note that this matrix is antisymmetric ($M^T = -M$) with a zero diagonal.</p> <p>However, we can model this election as a graph, with vertices corresponding to candidates and directed edges corresponding to the margin of votes between any pair of candidates. In particular, if there are $m$ total voters and $k$ ballots rank candidate $A$ over $B$ (so $m-k$ rank $B$ over $A$, for simplicity) with $k &gt; m-k$, then there is an edge from $A$ to $B$ with weight $k – (m-k)$. Conversely, if $m-k &gt; k$, then there is an edge from $B$ to $A$ with weight $m-k-k$ (and if $m-k = k$, no edge exists).</p> <p>(Feel free to skip this section if you’re comfortable so far) To see the graph and matrix representations of the problem more concretely, let’s imagine an election with four candidates, {A, B, C, D}, and 100 voters. The ballots break down as follows:</p> <table> <thead> <tr> <th>Voters</th> <th>Pref. 1</th> <th>Pref. 2</th> <th>Pref. 3</th> <th>Pref. 4</th> <th>Ballot ID</th> </tr> </thead> <tbody> <tr> <td>33</td> <td>A</td> <td>B</td> <td>C</td> <td>D</td> <td>1</td> </tr> <tr> <td>34</td> <td>B</td> <td>C</td> <td>A</td> <td>D</td> <td>2</td> </tr> <tr> <td>32</td> <td>C</td> <td>A</td> <td>B</td> <td>D</td> <td>3</td> </tr> <tr> <td>1</td> <td>A</td> <td>D</td> <td>B</td> <td>C</td> <td>4</td> </tr> </tbody> </table> <p>To determine the edge weight and direction between $B$ and $D$, for example, note that 99 ballots prefer $B$ to $D$ and 1 prefers $D$ to $B$, so we will have an edge from $B$ to $D$ with weight $99-1=98$ in our graph. In our matrix, $M_{B,D} = 98 = -M_{D,B}$. The graph representation is shown in Figure 1.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paradox-480.webp 480w,/assets/img/paradox-800.webp 800w,/assets/img/paradox-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paradox.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1: An example paradoxical election. For the edge from A to B, 66 voters prefer A to B and 34 B to A, so the weight is 66-34=32. </div> <p>There are some paradoxical results here: if we choose $A$ as the winner, note that ballots of type 1 and 3 both prefer $C$ to $A$, of which there are 65 total voters, so a majority prefer $C$ to $A$. However, 67 voters prefer $B$ to $C$, and 66 voters prefer $A$ to $B$, yielding a paradox. We could break this tie by counting who is most preferred over $D$, but that would make the last ballot a “dictator” ballot (in that one person’s vote is used to break a tie in favor of $A$, even though $C$ is preferred to $A$).</p> <p><strong>This reduction (/framework) can actually provide some good intuition on elections</strong>, as we can translate graph problems and their corresponding algorithms to voting systems, and vice versa. For example, suppose we wish to determine the <em>smallest set of candidates</em>, $S$ such that every candidate in $S$ would beat every candidate not in $S$ in a head-to-head matchup (economists call this set the Smith Set). Equivalently, we have a directed graph over all the candidates, and we wish to find the smallest set of vertices such that there are no incoming edges and there exists at least one path to every vertex outside of $S$. But this set is just a <a href="https://courses.grainger.illinois.edu/cs473/fa2011/lec/02_notes.pdf">source strongly connected component</a>: we have a set of vertices with a cycle amongst them, but there are no incoming edges.</p> <p>Tarjan’s algorithm can find us a source SCC in linear time, and we can check whether every vertex outside the set is reachable from every vertex in the set (equivalently, whether every other candidate outside the Smith Set is beatable) by running a Depth First Search (DFS) from one of the vertices in the source SCC (to prove this, start by noting that an edge exists between any two candidates except in the case of a tie, and use SCC properties from there). Figure 2 shows an example. In fact, any graph ordering algorithm, such as topological sorting, can be interpreted as a method for ordering candidates in some way (for every pair of candidates $A, B$ with $A$ before $B$ in the topological sort, $A$ must not lose to $B$).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/tarjan-480.webp 480w,/assets/img/tarjan-800.webp 800w,/assets/img/tarjan-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/tarjan.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2: Tarjan’s algorithm gives an ordering of SCCs. {A,B,C} is the Smith Set. </div> <p>In practice, finding the Smith Set can be used to winnow down a large pool of candidates to a smaller pool for a “runoff” election such that every candidate in the runoff would beat every candidate not in the runoff in a head-to-head election.</p> <p>It turns out that any deterministic method to select a winner (either by assigning weights to votes, or using some other method) can be broken with some paradoxical election. The paper I linked earlier proposes a randomized solution, which I discuss in the second half of the post.</p> <h2 id="ii-a-randomized-solution-for-paradoxes">II. A Randomized Solution for Paradoxes</h2> <p>To evaluate voting systems, we need some method for comparing two systems (i.e. some notion of voting system $X$, like first past the post, being better than voting system $Y$, like ranked-choice). First, a voting system is merely a function which takes an election profile $C$ of all ballots cast as input and outputs a single winning candidate. The exact profile, $C$, is one drawn from a distribution $D_C$, assigning probabilities to specific election profiles (this is natural; we tend to think of our own elections as having some level of noise). That is, each possible election profile has an associated probability. If there were three candidates and 10 voters, our distribution could assign a probability of 0.1 to (a profile of) 9 voters selecting {A, B, C} and 1 voter selecting {C, B, A}, for example. Every possible profile would have an assigned probability, which would give our distribution of election profiles.</p> <p>Let $P$ and $Q$ be specific voting systems, with $P(C) = x$, $Q(C) = y$. That is, on a given profile $C$, $P$ chooses $x$ as the winner, and $Q$ chooses $y$. The relative advantage of a voting system is just:</p> \[\text{Adv}_{D_C}(P,Q) = E_{D_C} (M_{x,y} / |C|)\] <p>It’s important to keep track of what’s fixed and what’s random here. The quantity inside expectation says: “given a particular profile $C$, what is the margin by which voters prefer P’s winner to Q’s winner, normalized by the total number of ballots.” If $P$ selects a winner less preferred than $Q$, then $M(x,y) &lt; 0$, and vice versa. We wish to calculate this quantity over all profiles, and thus take an expectation with respect to the distribution of profiles. More simply, P and Q give winners $x$ and $y$ for a particular profile $C$. We can calculate a “margin” $M(x,y)$ giving the number of voters who prefer $x$ to $y$, as well as the probability that $C$ is the actual profile. We now have two quantities: a margin and an associated probability, so we can take the expected margin between voting systems $P$ and $Q$. Intuitively, the relative advantage represents how many voters prefer $P$’s winner to $Q$’s winner in an average election.</p> <p>Then, $P$ is as good as or better than $Q$ if $\text{Adv}<em>{D_C}(P,Q) \geq 0$, and $P$ is optimal iff for every other voting system $Q$, $\text{Adv}</em>{D_C}(P,Q) \geq 0$.</p> <p>Given this method for comparing voting systems, we can now try to construct the optimal voting system. To do so, we’ll borrow some ideas from game theory. In particular, let the margin matrix $M_{x,y}$ be a payoff matrix and let voting systems $P$ and $Q$ be players in a two player game on this matrix. So, if $P$ selects $x$ as the winner and $Q$ selects $y$, $Q$ “pays” $M(x,y)$ to $P$. An optimal voting system $P_{OPT}$ would have non-negative expectation no matter what strategy is used by a competing voting system. The expected payoff of a voting system $Q$ to $P$ is:</p> \[\sum_{x,y} p_x q_y M(x,y)\] <p>If $Q$ is a deterministic voting system (i.e. $q_y = 1$ for one candidate, $0$ for all others), then $P$ can always just select the candidate which maximizes this payoff, and always do at least as well as $Q$ (i.e. never have more voters prefer $y$ over $x$ — in the worst case, $P$ and $Q$ just choose the same candidate).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pq-480.webp 480w,/assets/img/pq-800.webp 800w,/assets/img/pq-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/pq.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3: If Q chooses a column deterministically, P can maximize payout within that column by selecting a row and always at least break even with Q. </div> <p>But $M$ is anti-symmetric, so the column player and row player (or $P$ and $Q$) are interchangeable. <a href="https://www.cs.unc.edu/~lazebnik/fall10/lec11_game_theory2.pdf">The optimal strategy is a mixed strategy</a>, in which $P$ assigns each candidate a probability of being chosen as the winner and selects a winner according to these probabilities (for two player games, this equilibrium is the Nash equilibrium). <a href="https://sites.fas.harvard.edu/~libcs124/CS/lec16.pdf">This optimal mixed strategy can be calculated using linear programming</a>, and no mixed strategy can beat this strategy in expectation (follows from minimax).</p> <p>That was a lot, and if you want a better explanation, I’d encourage you to look at the paper, which goes into much more depth than I do here. They also write a number of simulations comparing the randomized system to other voting systems for various election profile distributions. The basic idea, though, is simple: no voting system can beat a randomized voting system in expectation.</p> <p>There are some interesting extensions of this study worth exploring, such as how the optimal strategy changes for different distributions of election profiles (in reality, we probably wouldn’t expect a uniform distribution over profiles), under what constraints a deterministic solution is best, and the characteristics of bad voting systems — i.e. which voting systems most often lose to random?</p> <p>It feels as though there’s something inherently undemocratic here, as democracy shouldn’t leave election winners to chance. I’d agree, and I think there are practical benefits to a well-designed, (possibly) paradoxical/unfair system that everyone understands and trusts. Maybe that system is a two-party system, maybe it’s ranked-choice as I discussed in the last post, or maybe it’s something else, but this framework for understanding voting can help guide our decisions.</p> <p>That being said, I think it’d be interesting to implement a randomized voting systems in certain circles (such as selecting leaders of scientific communities?) as test runs.</p> <h2 id="iii-addendum-2025">III. Addendum (2025)</h2> <p>I ended up simulating the randomized voting system above (with my classmate Sahana Srinivasan), using both some toy distributions and some real-world(!) election profiles from American cities in a class project for Ariel Procaccia’s <a href="https://econcs.seas.harvard.edu/class/cs-238-dpi-612-optimized-democracy">CS 238</a> class, which I highly recommend. If you’re really interested, you can attempt to scour our <a href="https://github.com/shuvom-s/cs238project/tree/main">code</a>.</p>]]></content><author><name></name></author><category term="blog-posts"/><category term="statistics"/><category term="ethics"/><category term="law"/><summary type="html"><![CDATA[Are our voting systems paradoxical?]]></summary></entry><entry><title type="html">Democracy and the Central Limit Theorem</title><link href="https://shuvom-s.github.io/blog/2020/democracy-and-the-central-limit-theorem/" rel="alternate" type="text/html" title="Democracy and the Central Limit Theorem"/><published>2020-05-15T00:00:00+00:00</published><updated>2020-05-15T00:00:00+00:00</updated><id>https://shuvom-s.github.io/blog/2020/democracy-and-the-central-limit-theorem</id><content type="html" xml:base="https://shuvom-s.github.io/blog/2020/democracy-and-the-central-limit-theorem/"><![CDATA[<p>Note: This post was originally published on <a href="https://randomquadwalks.com/2020/05/15/democracy-and-the-central-limit-theorem/">my old blog</a>.</p> <p>In the spirit of trying to reframe legal and philosophical questions through statistics and computer science frameworks, I thought I’d write about some parallels I see between democracy (or lack thereof) and probability. The law of large numbers and central limit theorem are ubiquitous, so it should be no surprise that we leverage it in so many algorithms and applications (like variational inference, maximum likelihood, etc.). At its core, democracy is an application of the central limit theorem: averaging together the many (possibly extreme) preferences of people guarantees some “average” preference which is perhaps not optimal, but certainly not terrible.</p> <p>Democracy is an average of preferences, but, in keeping with the previous post, let’s consider it an average of ethical frameworks as well (people’s preferences are guided by their ethical views, so this is a reasonable assumption).</p> <p>Suppose we had some method for scoring people’s “goodness” and “badness.” You and I may have different notions of what it means to be good, but our scores will almost certainly be correlated; we both think, hopefully, that it’s bad to murder and good to be kind to others. For the sake of this post, let’s stick to one scoring method, which we’ll call $S$ (every person $i$ may have their own method $S_i$, but the expected correlation between scoring methods is quite high).</p> <p>Let’s now define a distribution $D_S$ of people over these scores. That is, given a random person in the world, what’s the probability that they are at least as good as some score $x$? This will just be $P(p_i \geq x) = 1 - F_{D_S}(x)$.</p> <p>Note that with one random draw from this distribution, we are more likely to get extreme values, but, as the Central Limit Theorem tells us, as we draw more and more samples, the average will converge to the distribution’s true average (and the variance will asymptote towards zero).</p> <p>This provides us with a basis for understanding democracy; we average the (possibly extreme) preferences of many, many people with:</p> <ol> <li>The assumption that extreme preferences are more likely to be worse for society than average preferences.</li> <li>A belief that the average human being is good.</li> </ol> <p>I’d argue that these are two reasonable assumptions. First, note that one bad apple can spoil the lot. That is, it only takes one person with extreme preferences to ruin the lives of thousands of people with average preferences. A great example of this would be North Korea, where Kim Jong-Un single handedly has the power to enact any law he’d like (and unfortunately exercises this power often). In everyday life, mass shootings carried out by one person can ruin the lives of hundreds. More mathematically, suppose 1 out of every 100 people is a psychopath and scores extremely low on some goodness rating (so probability that some random person is worse than this person is 1%!).</p> <p>Dictatorships, which are analogous to one random draw from $D_S$, would give a 1% chance of selecting someone this bad, and that’s not even considering the lack of accountability that may further corrupt the person’s morals! Democracies, on the other hand, would almost surely prevent this possibility. So, in any functional society, it’s necessary to protect the masses against the tendencies of an extreme few.</p> <p>Why, then, do democracies fail to elect good leaders sometimes? A few theories may help explain:</p> <ol> <li> <p>CLT only holds under IID assumptions. That is, each voter must be drawn from the same distribution. This distribution is not static; it can be influenced by media, family, friends, etc., but on the day of the election, each voter is an IID draw from the distribution. In reality, this is sometimes not the case. Voter suppression (such as not letting certain minorities vote), election rigging, and misinformation can skew this distribution and/or violate the independence assumption.</p> </li> <li> <p>Suppressing others’ votes is probably correlated to psychopathic tendencies (or scoring “bad” on the scale), so the “bad” people are oversampled relative to the “good” people.</p> </li> </ol> <p>It’s no surprise that these ideas are not new; in fact <a href="https://philpapers.org/archive/GOOTPO-8.pdf">this paper</a> by two philosophers explores applications of <a href="https://en.wikipedia.org/wiki/Condorcet%27s_jury_theorem">Condorcet’s Jury Theorem</a>. The theorem is simple and follows from basic probability (some binomial sums):</p> <p>“Suppose voters have to decide between one incorrect and one correct option (imagine a jury, where there is a true verdict). If p, the probability of voting correctly, is greater than ½, then in the limit, the probability that the voters choose the correct decision is 1. If p is less than ½, then in the limit, the probability that they vote correctly is 0.”</p> <p>There are some interesting consequences of this theorem. For example, with 100 million voters and a 50.1 percent chance of a voter voting correctly (suppose “yes” for proposition A), there’s practically no measurable difference (i.e. p-value « 0.01) between 51% of voters choosing proposition A and 70% of voters choosing A. This seems particularly bizarre, because we traditionally think that a larger vote share gives the winning candidate a stronger mandate. If we think a 70-30 margin is a landslide, then what’s to say a 51-49 margin is not? After all, in elections, one could argue that our null hypothesis is the two candidates being equally popular and that the election is testing that hypothesis (what’s the point of an election if the null is that one candidate is preferred?).</p> <p>The upshot of all this is that in order for democracies to work, we need voting systems in which each individual vote <em>actually</em> reflects the voter’s preferences. Right now, we assume that a Bernoulli draw for each voter between two candidates (probability $p$ of choosing A; $n$ independent draws) is the correct model, but there are probably better systems out there. We’re forcing people to approximate their ethical frameworks and policy preferences on a ${0,1}$ support!</p> <p>One possibility could be a ranked-choice voting model, which would expand the support of the distribution. That is, if we have, say, 5 candidates, with 5 points to first choice, 4 points to second choice, and so on, we’d have a much larger support (120 possible orderings of the candidates). Each voter could then choose to align their preferences from one of 120 possible scores, which would enable us to capture more information about preferences from each vote. See the diagrams in the appendix for a more rigorous explanation.</p> <h2 id="appendix">Appendix</h2> <h3 id="diagram-for-ranked-choice-voting">Diagram for Ranked Choice Voting</h3> <p>I’ve included a couple diagrams below to clarify how ranked choice voting better approximates the average preferences. Suppose for simplicity that there are only two axes, healthcare and economy, along which everyone aligns (say more positive is more privatization, more negative is more centralization).</p> <p>Note that in forcing voters to choose between two candidates, we constrain the national vote to a line drawn between these two candidates (see Figure 1). The election therefore becomes a projection of the true national preference, $V_\text{avg}$, onto this line between candidates A and B.</p> <p>With ranked choice voting and more candidates (suppose three, in Figure 2), each vote can be a weighted sum of all these candidates’ positions on healthcare and economy, so the space of possible solutions (or final vote tallies) is less constrained. The national vote is then constrained to a triangle defined by these three points, and the election is a projection of $V_\text{avg}$ onto this triangle. Each voter $V_i$’s vote is a weighted average of the three candidates, and the election is an average of all $V_i$.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/twocandidate-480.webp 480w,/assets/img/twocandidate-800.webp 800w,/assets/img/twocandidate-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/twocandidate.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1: Two-candidate single voter system is a projection onto a line. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/threecandidate-480.webp 480w,/assets/img/threecandidate-800.webp 800w,/assets/img/threecandidate-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/threecandidate.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2: Ranked choice voting offers greater range of potential preference combinations. </div> <h3 id="discussion-of-dictator-model">Discussion of Dictator Model</h3> <p>Here we made the assumption that dictators are akin to one random draw from the national distribution of preferences, but relaxing this assumption actually yields even worse results for dictatorships. There’s some negative selection here; dictators are more likely to have narcissistic and psychopathic tendencies to reach the positions they reached, so sampling a dictator from a population is probably even more skewed to the undesirable portions of the distribution.</p> <h3 id="other-assumptions">Other Assumptions</h3> <p>Several people have pointed out that if the average voter has “bad” preferences, democracies will also fail. This is certainly true, and I thought about including it, but the focus of this article was a theoretical justification for democracy assuming the average voter has “good” preferences. To me, these are two separate (but both very important!) questions, as democracy hinges on two assumptions: (1) voters are “good” and (2) given that voters are “good,” the voting system approximates their preferences well.</p> <p>There are a variety of problems that can complicate the assumptions in (1), including, but not limited to, the average voter being “bad,” herd mentality, tyranny of the majority, and misinformation.</p> <p>There are also some issues with the ranked choice model’s feasibility in the United States, given the existing two-party structure (and all sorts of other issues). Again, this is not an endorsement for ranked choice voting’s implementation but rather a (more) mathematical exploration of the differences between voting systems.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="statistics"/><category term="ethics"/><category term="law"/><summary type="html"><![CDATA[Is democracy just an average of preferences?]]></summary></entry><entry><title type="html">A Statistical Analogy for Ethics</title><link href="https://shuvom-s.github.io/blog/2020/a-statistical-analogy-for-ethics/" rel="alternate" type="text/html" title="A Statistical Analogy for Ethics"/><published>2020-05-10T00:00:00+00:00</published><updated>2020-05-10T00:00:00+00:00</updated><id>https://shuvom-s.github.io/blog/2020/a-statistical-analogy-for-ethics</id><content type="html" xml:base="https://shuvom-s.github.io/blog/2020/a-statistical-analogy-for-ethics/"><![CDATA[<p>Note: This post was originally published on <a href="https://randomquadwalks.com/2020/05/10/a-statistical-analogy-for-ethics/">my old blog</a>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ethics-480.webp 480w,/assets/img/ethics-800.webp 800w,/assets/img/ethics-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ethics.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>I’ve grown a little frustrated in how morality and moral law are often misused in public debate.</p> <p>To start, let’s consider what a moral framework entails. In some sense, moral philosophy is merely a model to determine whether an action is right or wrong. Different frameworks specify exact models for understanding right and wrong; some consider the output a spectrum; others consider it a binary; others even say it’s impossible to interpret any output.</p> <p>It’s important to note that the <em>model itself</em> is distinct from the <em>political legitimacy</em> of the model (i.e. how well a model is accepted as legitimate by the government). Our federal laws, for example, realize a philosophical model with strong political legitimacy, but federal laws are amended and changed when that model does not coincide with some other, “truer” philosophical framework.</p> <p>This notion may seem excruciatingly obvious, but you’ll frequently hear accusations of setting “arbitrary moral guidelines” or appeals to the constitution because it was “willed by the people.” These attacks only undermine the political legitimacy of the chosen moral framework, not the internal validity.</p> <p>It’s equally frustrating when critics of a certain moral framework find tiny, irrelevant exceptions as conclusive disproofs. Just as we wouldn’t discard a machine learning model because of occasional errors, we shouldn’t discard philosophical frameworks because we can find exceptions to them.</p> <p>Instead, a more convincing critique takes issue with the assumptions made by the model, similar to how we’d criticize a poorly applied machine learning method.</p> <p>Moreover, any philosophical framework necessarily makes simplifying assumptions and prescribes categorical laws. Utilitarians may shake their fists at me and argue that utilitarianism maximizes good, but I would argue that any framework attempts to maximize good, so this naive utilitarianism fails to prescribe action. Without an exact set of ascribed utilities to actions, I cannot guide my action to “maximize good.” And once you give me this set of weights, it’s almost certainly possible to find a counterexample.</p> <p>Reaching moral skepticism from these observations is even more preposterous: that would be equivalent to saying “because physics is sometimes contradictory, we should discard our entire conception of the universe.”</p> <p>To see how all of these pieces fit together, let’s imagine morality as a sort of mathematical problem. Imagine we have a set of (many, many) axes which contextualize each action we take (e.g. the time of the action, who is there, their positions in society, etc.—every possible degree-of-freedom of a given action). Each action we take is a dot in this large grid, and we wish to classify these dots as moral (“1”) or immoral (“0”) decisions (or a regression problem if you believe morality is a spectrum). There exists a true classification (although this is debatable), and we want to find a model that best approximates this true classification.</p> <p>We can, therefore, think of categorical laws such as “never steal” as analogous to simpler models like linear regression. We can add conditions to increase model complexity, such as “never steal unless the person is six feet tall and has seven kids.” More generally, model complexity, bias, and variance in machine learning can be analogized to moral frameworks:</p> <ol> <li> <p><strong>Model Complexity</strong>: As we increase model complexity, we lose interpretability, which is especially concerning in moral philosophy (which serve as interpretable guides to action), so we should penalize convoluted moral frameworks.</p> </li> <li> <p><strong>Interpretability</strong> Taken to an extreme, we can have vague and uninterpretable moral models, which, while perhaps technically “correct,” could be very “noisy” and uninterpretable, paralyzing action. Many formulations of utilitarianism fall under this umbrella, in which someone blindly asserts that the best thing to do is to do the most good and then quantifies some metric like maximizing total happiness. Absent a concrete method to achieve this ideal, utilitarianism can be used to justify almost any policy with some set of weights. Think about it: when is the last time a politician made an argument and didn’t invoke some form of “everyone will be better off?” Instead, if we trade some model complexity for categorical laws (which international law aims to do, for example), we may often achieve better results, even by the utilitarian’s own metrics, similar to how we often aim to achieve the optimal “tradeoff” between bias and variance in selecting machine learning models. Perhaps going down this path may lead one to <a href="https://en.wikipedia.org/wiki/Rule_utilitarianism">rule utilitarianism</a>.</p> </li> <li> <p><strong>Bias and Variance</strong>: While simple moral models may have high bias (i.e. inaccurate in many scenarios), complex moral models will have higher variance as they paralyze action. For example, many religious texts have complex moral guidelines, so we defer the interpretation to our denomination and priests, yielding higher model variance. On the other hand, simple moral guidelines like the Ten Commandments might incur significant contradictions, but are far more actionable.</p> </li> </ol> <p>While small and relatively trivial, this example illustrates how we can cross-apply other fields to guide understanding of ethical systems.</p> <p>It’s not a secret that different fields often employ their own jargon, which makes their ideas (slightly) inaccessible to other fields, but connecting ideas across fields (e.g. bias-variance tradeoff and moral philosophy) can illuminate how the same logic often underlies different results.</p>]]></content><author><name></name></author><category term="blog-posts"/><category term="statistics"/><category term="ethics"/><category term="law"/><summary type="html"><![CDATA[Reasoning about ethics via an analogy]]></summary></entry><entry><title type="html">Ramblings on Inference and Doubt</title><link href="https://shuvom-s.github.io/blog/2018/ramblings-on-inference-and-doubt/" rel="alternate" type="text/html" title="Ramblings on Inference and Doubt"/><published>2018-02-01T00:00:00+00:00</published><updated>2018-02-01T00:00:00+00:00</updated><id>https://shuvom-s.github.io/blog/2018/ramblings-on-inference-and-doubt</id><content type="html" xml:base="https://shuvom-s.github.io/blog/2018/ramblings-on-inference-and-doubt/"><![CDATA[<p>Note: This post was originally published on <a href="https://shuvom.quora.com/Ramblings-on-Inferences-and-Doubt">my old blog on Quora</a>. Please forgive my teenage edginess.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/doubt-480.webp 480w,/assets/img/doubt-800.webp 800w,/assets/img/doubt-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/doubt.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>I’ve been contemplating over the past few months over religion, God’s existence (or non-existence), and the philosophical implications and underpinnings of free will and such. One school of thought that caught my attention was the Lokayata school of Vedic philosophy. I was introduced to this line of thought in Amartya Sen’s <em>The Argumentative Indian</em> and found it compelling given its atheistic inclinations and grounding of all knowledge in empiricism, a feature rather uncommon in religious philosophies.</p> <p>Central to Lokayata philosophy is the constant questioning of all propositions and inferences. “No inference can be made without doubt,” is the motto of Lokayata and arguably the basis for most of its philosophical advances that follow. However, there seems to be an unavoidable paradox within: the statement itself seems to be an inference made from experience and perception but expresses itself without doubt.</p> <ol> <li> <p>Suppose the statement “no inference can be made without doubt” is true.</p> </li> <li> <p>Since (a) this statement is itself an inference and (b) implicit within any linguistic description of experience and/or perception is an inference that language can describe the world (i.e. when I say “the jacket is red” I am assuming implicitly that it is possible to convey the experience of the color “red” through English), there exists doubt over the original statement.</p> </li> <li> <p>Thus, the statement should be amended to “it is possible that no inference can be made without doubt.”</p> </li> <li> <p>This also implies the statement “it is possible that an inference can be made without doubt.”</p> </li> <li> <p>Since possibility necessitates experience at some point in time*, this means that an inference must have been made without doubt at some point in time, thus contradicting (1).</p> </li> </ol> <p>*This step is sketchy. As the argument goes, since any perception of existence must be based in experience, if $X$ possibly exists, $X$ must have existed at some time. For example, if I say “UFOs possibly exist” this implies that UFOs must have existed at some point in time, as there would otherwise be no experience for us to base our perception off of. It is not necessary that the subject experienced UFOs or that humanity ever did; however, since the perception of UFOs didn’t appear out of nowhere, we must be able to trace back the history of this perception to some time or point where it existed. There is an inherent problem I see with this argument: (a) it is possible for a human to perceive something that never existed (say, 25th dimension). We may be able to imagine a world in which the perceived object exists (think abstract physics or mathematics!) without having any ground to base this perception. This is precisely the reason why our conceptions of God/higher dimensions are hazy.</p>]]></content><author><name></name></author><category term="blog-posts"/><category term="philosophy"/><category term="science"/><summary type="html"><![CDATA[Is empiricism infallible?]]></summary></entry></feed>